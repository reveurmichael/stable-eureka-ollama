def compute_reward(self, pos, action, state):
    # Define temperature parameters for normalization
    distance_temp = 0.01
    velocity_temp = 0.01

    # Compute individual reward components
    distance_reward = pos[0]
    velocity_reward = state[2]  # Using normalized velocity in the x-direction

    # Apply transformations for smooth learning
    distance_reward_transformed = np.exp(distance_temp * distance_reward) - 1
    velocity_reward_transformed = np.exp(velocity_temp * velocity_reward) - 1

    # Compute total reward
    total_reward = distance_reward_transformed + velocity_reward_transformed

    # Create a dictionary of individual reward components
    individual_reward = {
        'distance_reward': distance_reward,
        'velocity_reward': velocity_reward,
        'distance_reward_transformed': distance_reward_transformed,
        'velocity_reward_transformed': velocity_reward_transformed
    }

    return total_reward, individual_reward