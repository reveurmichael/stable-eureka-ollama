```python
def compute_reward(self, pos, action, state):
    done = state[0] > (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP or self.game_over or pos[0] < 0
    
    total_reward = np.exp(-10 * abs(action).sum()) if not done else 100
    individual_rewards = {'distance_reached': int(pos[0]) * 1, 'velocity': state[2], 'leg_ground_contact': (state[-1] + state[-5]).mean()}

    return total_reward, individual_rewards
```

Explanation:
The reward function is designed to encourage the biped to move forward as quickly and efficiently as possible while avoiding falling down or getting stuck. The total reward is based on the magnitude of the action taken by the agent, with higher rewards for smaller actions (i.e., less movement). This encourages the agent to take small steps and adjust its movement accordingly.

When the episode ends (either because the biped has reached the end of the terrain or fallen off), a large reward is given to encourage exploration and reaching the goal. The individual rewards provide additional feedback on the agent's performance, with metrics for distance traveled, velocity, and leg ground contact.