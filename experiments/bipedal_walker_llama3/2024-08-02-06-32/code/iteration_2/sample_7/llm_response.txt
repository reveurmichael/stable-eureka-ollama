```python
def compute_reward(self, pos, action, state):
    hull_angle = (state[0] + math.pi) % (2 * math.pi)
    vertical_speed = -state[4]
    forward_speed = -state[2]

    step_reward = 0.1 if not self.game_over else -5
    reach_goal_reward = 10 if pos[0] > (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP else 0

    stable_reward = math.exp(-(hull_angle ** 2 + vertical_speed ** 2) / (10 ** 2))
    movement_reward = math.exp(forward_speed ** 2 / (1 ** 2))

    reward = step_reward + reach_goal_reward + stable_reward * movement_reward
    individual_reward = {
        'step': step_reward,
        'reach_goal': reach_goal_reward,
        'stable': stable_reward,
        'movement': movement_reward
    }
    return reward, individual_reward
```

Explanation: This code defines a reward function for the biped navigation task. The reward is designed to encourage the agent to move forward, stabilize its stance, and avoid falling or getting stuck. The step reward encourages the agent to take small steps while moving forward, with a penalty for falling or getting stuck. The reach goal reward gives a large bonus when the agent reaches the end of the terrain. The stable reward encourages the agent to maintain a stable stance by minimizing its angular deviation from the horizontal and vertical speed. The movement reward encourages the agent to move forward by maximizing its forward speed.