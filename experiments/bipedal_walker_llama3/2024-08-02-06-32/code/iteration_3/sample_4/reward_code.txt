def compute_reward(self, pos, action, state):
    speed = np.clip(state[2] + state[3], -1.0, 1.0)
    distance_to_end = (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP - pos[0]
    
    # Positive reward for reaching the end
    reward_component_end_reached = np.exp(np.tanh(distance_to_end / 5.0)) if distance_to_end < 0 else 0
    
    # Negative penalty for not moving forward
    reward_component_movement_penalty = -np.exp(-speed)
    
    # Negative penalty for falling down or getting stuck
    reward_component_fall_down_penalty = -1 if pos[0] < 0 or any(contact == 1.0 for contact in state[17:21]) else 0
    
    # Reward for not falling and reaching the end, normalized to [-1, 1]
    total_reward = (reward_component_end_reached + reward_component_movement_penalty + reward_component_fall_down_penalty) / 3
    
    return total_reward, {'end_reached': reward_component_end_reached, 'movement_penalty': reward_component_movement_penalty, 'fall_down_penalty': reward_component_fall_down_penalty}