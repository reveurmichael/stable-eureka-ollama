```python
def compute_reward(self, pos, action, state):
    hull_angle = state[0]
    vel_x = state[2]
    vel_y = state[3]
    
    # positive reward for moving forward (reaching the end)
    reward_forward = 0.5 * np.exp((pos[0] - TERRAIN_LENGTH) / 10.0)

    # penalty for falling down or getting stuck
    if pos[1] < 0:
        reward_fall = -100.0
    elif abs(vel_y) < 0.2:
        reward_stuck = -50.0
    else:
        reward_fall = 0.0
        reward_stuck = 0.0

    # penalty for not moving at all (not reaching the end)
    if vel_x == 0 and vel_y == 0:
        reward_idle = -20.0
    else:
        reward_idle = 0.0

    reward_total, individual_reward = self.get_individual_rewards(hull_angle)

    return reward_forward + reward_fall + reward_stuck + reward_idle + reward_total, individual_reward
```

Explanation: The goal is to encourage the agent to move forward and avoid falling down or getting stuck while reaching the end. This is achieved by a combination of positive rewards for moving forward and penalties for falling down or getting stuck. Additionally, there's a penalty for not moving at all (idle) to prevent the agent from staying in one position forever.