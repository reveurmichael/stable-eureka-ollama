def compute_reward(self, pos, action, state):
    reward = 0.0
    individual_reward = {'distance_reward': 0.0, 'velocity_reward': 0.0, 'orientation_reward': 0.0}

    if not self.game_over:
        distance_to_goal = TERRAIN_LENGTH - pos[0]
        distance_reward = np.exp(np.clip(distance_to_goal / (TERRAIN_LENGTH / 2), -10, 10) * TEMPERATURE)
        reward += distance_reward
        individual_reward['distance_reward'] = distance_reward

        velocity_reward = np.exp(action[0] ** 2 + action[1] ** 2 + action[2] ** 2 + action[3] ** 2 * TEMPERATURE)
        reward += velocity_reward
        individual_reward['velocity_reward'] = velocity_reward

    orientation_reward = np.exp(np.clip(state[0], -10, 10) * ORIENTATION_TEMPERATURE)
    reward += orientation_reward
    individual_reward['orientation_reward'] = orientation_reward

    return reward, individual_reward