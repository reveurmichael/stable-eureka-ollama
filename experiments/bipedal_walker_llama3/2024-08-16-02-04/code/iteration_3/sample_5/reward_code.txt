def compute_reward(self, pos, action, state):
    hull_angle = state[0]
    velocity_x = state[2]
    velocity_y = state[3]
    is_right_end_reached = pos[0] > (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP
    
    distance_reward = np.exp(np.clip(-abs(velocity_x), 0, 1))  # Smoothness and speed
    stability_reward = np.exp(np.clip(-abs(hull_angle), 0, 1))  # Stabilization
    end_reached_reward = np.exp(is_right_end_reached)  # Reaching the end
    
    reward = distance_reward + stability_reward + end_reached_reward
    individual_rewards = {'distance': distance_reward, 'stability': stability_reward, 'end_reached': end_reached_reward}
    
    return reward, individual_rewards