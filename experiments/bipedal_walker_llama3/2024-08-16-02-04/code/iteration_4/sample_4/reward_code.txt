def compute_reward(self, pos, action, state):
    hull_angle = (state[0] + math.pi) % (2 * math.pi)
    velocity = np.sqrt(state[2]**2 + state[3]**2)
    
    reward = -1.0  # penalty for not reaching the end
    if pos[0] > (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP:
        reward += 10.0  # bonus for reaching the end
    
    individual_reward = {'end': 0.0, 'velocity': np.exp(-np.abs(velocity)) * 5.0}
    
    return reward + sum(individual_reward.values()), individual_reward