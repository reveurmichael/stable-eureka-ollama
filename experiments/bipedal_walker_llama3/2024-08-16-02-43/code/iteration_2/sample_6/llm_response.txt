```python
def compute_reward(self, pos, action, state):
    distance_to_goal = pos[0] - TERRAIN_LENGTH + TERRAIN_GRASS * TERRAIN_STEP
    falling_down = pos[1] < 0 or any(l.ground_contact for l in self.legs)
    
    reward = np.exp(-distance_to_goal / REWARD_DISTANCE_SCALE) * (1 if not falling_down else -np.exp(-falling_down / REWARD_FALLING_SCALE))
    individual_reward = {"goal_distance": distance_to_goal, "falling_down": 0 if not falling_down else -1}
    
    return reward, individual_reward
```

Explanation:
The reward function aims to encourage the biped to reach the right end of the terrain and avoid falling down. It calculates a negative exponential decay based on the distance to the goal, with a scale factor `REWARD_DISTANCE_SCALE`. This encourages the agent to move closer to the goal. Additionally, it penalizes the agent for falling down by setting a reward equal to `-np.exp(-falling_down / REWARD_FALLING_SCALE)`, where `REWARD_FALLING_SCALE` is another scale factor. If the biped is not falling down, the reward remains positive and encourages further movement towards the goal.