def compute_reward(self, pos, action, state):
    hull_angle = (state[0] + math.pi) % (2 * math.pi) - math.pi
    vel_x = state[3]
    dist_to_goal = (pos[0] - TERRAIN_LENGTH) / TERRAIN_STEP

    reward = 0.5 * np.exp(1) * (np.cos(hull_angle) + np.abs(vel_x)) - 0.2 * np.exp(0.5) * dist_to_goal
    individual_reward = {'hull_alignment': 0.5 * np.exp(1) * (np.cos(hull_angle)),
                          'velocity': 0.5 * np.exp(1) * np.abs(vel_x),
                          'distance_to_goal': -0.2 * np.exp(0.5) * dist_to_goal}

    return reward, individual_reward