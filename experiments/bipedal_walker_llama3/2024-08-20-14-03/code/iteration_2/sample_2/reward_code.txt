def compute_reward(self, pos, action, state):
    hull_angle = (state[0] + math.pi) % (2 * math.pi) - math.pi
    vel_x = state[3]
    dist_to_goal = (pos[0] - TERRAIN_LENGTH) / TERRAIN_STEP
    
    # Scale down velocity component
    velocity_reward = 0.1 * np.abs(vel_x)
    
    # Scale up fitness score component
    fitness_score_reward = 0.5 * (1 + np.tanh(fitness_score))
    
    # Adjust distance to goal component to encourage exploration
    dist_to_goal_reward = -0.05 * np.exp(-dist_to_goal / 10000) * math.cos(hull_angle)
    
    reward = velocity_reward + fitness_score_reward + dist_to_goal_reward
    
    individual_rewards = {'hull_alignment': 0.5 * np.exp(1) * (np.cos(hull_angle)),
                          'velocity': velocity_reward,
                          'distance_to_goal': dist_to_goal_reward}
    
    return reward, individual_rewards