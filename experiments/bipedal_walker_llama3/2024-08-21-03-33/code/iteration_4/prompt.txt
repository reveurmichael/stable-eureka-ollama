You are a reward engineer writing effective reward functions for reinforcement learning tasks. Your goal is to create a reward function to help the agent learn the task described in text. Use relevant environment variables as inputs. Example signature:
```python
def compute_reward(self, ...):
    ...
    return reward, {}
```
Do not use type hints. Return type: float, Dict[str, float].
Coding instructions: The reward function output should include:
1. Total reward (float)
2. Dictionary of individual reward components
Format as a Python code string: "```python ... ```". Tips:
1. Normalize rewards using transformations like np.exp. Introduce temperature parameters for each transformation.
2. Ensure input types match expected types.
3. Use only self. attributes from the environment class definition and input variables.
4. If you create a self var inside the compute_reward function you must consider that it is not previously defined in the environment class:
    e.g. if hasattr(self, 'var') is False: self.var = 0 else self.var += 1
5. No new input variables.
6. The python code must begin with: "```python ... ```"
7. Pass self as the first argument.
8. Do not compute fitness_score components.
9. Only create the reward function, do not create more functions.
10. Try to make everything smooth (when possible).
Provide the function code only, followed by a brief explanation (max 50 words).
Task description: The goal is to make a biped navigate a 2D environment:
- The biped starts standing at the left end of the terrain with the hull horizontal, and both legs in the same position with a slight knee angle
- The biped has to reach the right end of the terrain and avoid falling down, getting stuck while surpassing obstacles (e.g. gaps, steps, stairs)
- We do not want the biped to get stuck in a certain position, he should keep moving forward, as far as possible
- The biped should go as fast as possible, so it gets to the end in less steps. The fewer episode steps, the better if getting to the end
- It is more important to reach the end than being super smooth, because the agent could decide not to move to avoid falling down, which is not the goal
Environment code:
def step(self, action: np.ndarray):
    assert self.hull is not None

    # self.hull.ApplyForceToCenter((0, 20), True) -- Uncomment this to receive a bit of stability help
    control_speed = False  # Should be easier as well
    if control_speed:
        self.joints[0].motorSpeed = float(SPEED_HIP * np.clip(action[0], -1, 1))
        self.joints[1].motorSpeed = float(SPEED_KNEE * np.clip(action[1], -1, 1))
        self.joints[2].motorSpeed = float(SPEED_HIP * np.clip(action[2], -1, 1))
        self.joints[3].motorSpeed = float(SPEED_KNEE * np.clip(action[3], -1, 1))
    else:
        self.joints[0].motorSpeed = float(SPEED_HIP * np.sign(action[0]))
        self.joints[0].maxMotorTorque = float(
            MOTORS_TORQUE * np.clip(np.abs(action[0]), 0, 1)
        )
        self.joints[1].motorSpeed = float(SPEED_KNEE * np.sign(action[1]))
        self.joints[1].maxMotorTorque = float(
            MOTORS_TORQUE * np.clip(np.abs(action[1]), 0, 1)
        )
        self.joints[2].motorSpeed = float(SPEED_HIP * np.sign(action[2]))
        self.joints[2].maxMotorTorque = float(
            MOTORS_TORQUE * np.clip(np.abs(action[2]), 0, 1)
        )
        self.joints[3].motorSpeed = float(SPEED_KNEE * np.sign(action[3]))
        self.joints[3].maxMotorTorque = float(
            MOTORS_TORQUE * np.clip(np.abs(action[3]), 0, 1)
        )

    self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)

    pos = self.hull.position
    vel = self.hull.linearVelocity

    for i in range(10):
        self.lidar[i].fraction = 1.0
        self.lidar[i].p1 = pos
        self.lidar[i].p2 = (
            pos[0] + math.sin(1.5 * i / 10.0) * LIDAR_RANGE,
            pos[1] - math.cos(1.5 * i / 10.0) * LIDAR_RANGE,
        )
        self.world.RayCast(self.lidar[i], self.lidar[i].p1, self.lidar[i].p2)

    state = [
        self.hull.angle,  # Normal angles up to 0.5 here, but sure more is possible.
        2.0 * self.hull.angularVelocity / FPS,
        0.3 * vel.x * (VIEWPORT_W / SCALE) / FPS,  # Normalized to get -1..1 range
        0.3 * vel.y * (VIEWPORT_H / SCALE) / FPS,
        self.joints[0].angle,
        # This will give 1.1 on high up, but it's still OK (and there should be spikes on hiting the ground, that's normal too)
        self.joints[0].speed / SPEED_HIP,
        self.joints[1].angle + 1.0,
        self.joints[1].speed / SPEED_KNEE,
        1.0 if self.legs[1].ground_contact else 0.0,
        self.joints[2].angle,
        self.joints[2].speed / SPEED_HIP,
        self.joints[3].angle + 1.0,
        self.joints[3].speed / SPEED_KNEE,
        1.0 if self.legs[3].ground_contact else 0.0,
    ]
    state += [l.fraction for l in self.lidar]
    assert len(state) == 24

    self.scroll = pos.x - VIEWPORT_W / SCALE / 5

    terminated = False
    if self.game_over or pos[0] < 0:
        terminated = True
    # stable-eureka: this is the function you must implement!
    reward, individual_reward = self.compute_reward(pos, action, state)

    fitness_score = self.compute_fitness_score(pos, action, state)

    if pos[0] > (TERRAIN_LENGTH - TERRAIN_GRASS) * TERRAIN_STEP:
        terminated = True

    if self.render_mode == "human":
        self.render()

    info = individual_reward.update({'fitness_score': fitness_score})

    return np.array(state, dtype=np.float32), reward, terminated, False, info

Reward reflection:
We trained a RL policy using the provided reward function code and tracked (on evaluation on several points of the training stage) the values of individual reward components, along with global policy metrics such as fitness scores and episode lengths. Maximum, mean, and minimum are provided:
   progress: [4.026162243874663e-14, 3.646988471136363e-14, 3.743043449649295e-14, 3.893086935641043e-14, 3.961963898503831e-14, 3.893008181549378e-14, 4.2350548300893656e-14, 4.545608256083657e-14, 4.450915529006797e-14, 4.3947666490884736e-14]. Max: 4.545608256083657e-14 - Mean: 4.0790598444622865e-14 - Min: 3.646988471136363e-14 
   moving_forward: [7.5, 10.0, 9.5, 24.5, 32.5, 17.5, 157.0, 1570.0, 829.5, 1561.5]. Max: 1570.0 - Mean: 421.95 - Min: 7.5 
   penalty: [9.97030597865573, 9.923536394834523, 9.762410837113965, 10.052877578437283, 10.060602987110546, 10.054560749828699, 9.898168693482864, 9.822311904132357, 9.973527461588404, 9.946430457532413]. Max: 10.060602987110546 - Mean: 9.946473304271679 - Min: 9.762410837113965 
   fitness_score: [-53.73733099672178, -87.65890318063619, -100.94304679114379, -97.37764780046763, -99.2716292930478, -90.50122575847797, -87.88495409587046, -91.21907346701636, -83.68026484844091, -68.15180598416924]. Max: -53.73733099672178 - Mean: -86.0425882215992 - Min: -100.94304679114379 
   reward: [9.970305790811835, 9.923536407324718, 9.762410780967912, 10.052877551803249, 10.060602900048252, 10.054560870616115, 9.898168724277639, 9.822311892392463, 9.973527471374837, 9.94643039663788]. Max: 10.060602900048252 - Mean: 9.94647327862549 - Min: 9.762410780967912 
   episode_length: [1600.0, 1600.0, 1600.0, 1600.0, 1600.0, 1600.0, 1600.0, 1600.0, 1600.0, 1600.0]. Max: 1600.0 - Mean: 1600.0 - Min: 1600.0 

Please analyze the policy feedback and provide an improved reward function to better solve the task. Tips for analyzing feedback:
1. If a reward component's values are nearly identical, or it increases when it should decrease (minimize) and vice versa, consider the following options:
    a. Change its scale or temperature parameter.
    b. Re-write the component.
    c. Discard the component.
2. If a component's magnitude is significantly larger, re-scale it.
3. You want to maximize the fitness score as it is the ground truth evaluator.
4. You want to maximize positive reward components values during training and minimize negative reward components values.
5. If the fitness score is not improving during training, try to change the reward function.
Analyze each existing reward component first, then write the new reward function code proposal.Stable-Eureka best iteration  (you should modify it!): 
    # Generated code by stable-eureka
    def compute_reward(self, pos, action, state):
        distance_to_goal = abs(pos[0] - TERRAIN_LENGTH)
        moving_forward = not self.game_over and (pos[0] < 0 or pos[0] > TERRAIN_GRASS * TERRAIN_STEP)
    
        reward = np.exp(-(distance_to_goal ** 2) / 1000) * (1 if moving_forward else 0.5) - 0.01 * (state[6] + state[13])
        
        individual_reward = {"progress": np.exp(-(distance_to_goal ** 2) / 1000), "moving_forward": 1 if moving_forward else 0, "penalty": -0.01 * (state[6] + state[13])}
        
        return reward, individual_reward

The next training will take the best model weights so it reuses some of the relevant information from the previous training!
Your reward code is: 