{
    "0": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right\n        distance_reward = pos.x\n        distance_reward = np.exp(distance_reward / 400.0) - 1\n    \n        # Stability reward: penalizing large angle and angular velocity\n        angle_penalty = -np.abs(state[0])\n        angle_penalty = np.tanh(angle_penalty / 0.5)  # More stringent scaling\n    \n        angular_velocity_penalty = -np.abs(state[1])\n        angular_velocity_penalty = np.tanh(angular_velocity_penalty / 0.5)  # Increased impact with stringent scaling\n    \n        # Smoothness reward: penalizing high-speed actions\n        action_penalty = -np.sum(np.abs(action)) / len(action)\n        action_penalty = np.tanh(action_penalty / 0.2)  # Reduced impact for less stringent scaling\n    \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar])\n        obstacle_penalty = np.exp(obstacle_penalty)  # Exponential scaling for higher penalty with increased weight\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.2 * angle_penalty\n                  + 0.1 * angular_velocity_penalty\n                  + 0.3 * action_penalty\n                  + 0.5 * obstacle_penalty)  # Higher weight given to distance reward and obstacle penalty\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty\n        }\n    \n        return reward, individual_reward",
        -24.223881282273666
    ],
    "1": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right (scaled)\n        distance_reward = pos.x / 200.0\n    \n        # Stability reward: penalizing large angle and angular velocity (reduced magnitude)\n        angle_penalty = -np.abs(state[0]) * 0.05\n        angular_velocity_penalty = -np.abs(state[1]) * 0.01\n    \n        # Smoothness reward: penalizing high-speed actions (scaled)\n        action_penalty = -np.sum(np.abs(action)) / len(action) * 0.1\n    \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.2\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.1 * angle_penalty\n                  + 0.05 * angular_velocity_penalty\n                  + 0.15 * action_penalty\n                  + 0.3 * obstacle_penalty)  # Higher weight given to distance reward and obstacle penalty\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty\n        }\n    \n        return reward, individual_reward",
        -9.90103017578458
    ],
    "2": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right (scaled)\n        distance_reward = pos.x / 200.0\n        \n        # Stability reward: penalizing large angle and angular velocity (reduced magnitude)\n        angle_penalty = -np.abs(state[0]) * 0.005\n        angular_velocity_penalty = -np.abs(state[1]) * 0.001\n        \n        # Smoothness reward: penalizing high-speed actions (scaled)\n        action_penalty = -np.sum(np.abs(action)) / len(action) * 0.05\n        \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.5\n        \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.1 * angle_penalty\n                  + 0.05 * angular_velocity_penalty\n                  + 0.2 * action_penalty\n                  + 0.6 * obstacle_penalty)  # Higher weight given to distance reward and obstacle penalty\n        \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty\n        }\n        \n        return reward, individual_reward",
        -100.53957571691461
    ],
    "3": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right (scaled)\n        distance_reward = pos.x / 200.0 * 10\n        \n        # Stability reward: penalizing large angle and angular velocity (reduced magnitude)\n        angle_penalty = -np.abs(state[0]) * 0.001\n        angular_velocity_penalty = -np.abs(state[1]) * 0.0005\n        \n        # Smoothness reward: penalizing high-speed actions (scaled)\n        action_penalty = -np.sum(np.abs(action)) / len(action) * 0.05\n        \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.1\n        \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.2\n                  + angle_penalty * 0.05\n                  + angular_velocity_penalty * 0.05\n                  + action_penalty * 0.05\n                  + obstacle_penalty * 0.8)\n        \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty\n        }\n        \n        return reward, individual_reward",
        -38.709126533851
    ],
    "4": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right (scaled)\n        distance_reward = pos.x / 200.0 * 10\n        \n        # Stability reward: penalizing large angle and angular velocity (reduced magnitude)\n        angle_penalty = -np.abs(state[0]) * 0.005\n        angular_velocity_penalty = -np.abs(state[1]) * 0.0025\n        \n        # Smoothness reward: penalizing high-speed actions (scaled)\n        action_penalty = -np.sum(np.abs(action)) / len(action) * 0.02\n        \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_reward = -np.min([l.fraction for l in self.lidar]) * 0.05\n        \n        # Fitness score reward: incentivizing high fitness scores\n        fitness_reward = np.clip(state[2], 0, 1) * 10\n        \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.3\n                  + angle_penalty * 0.05\n                  + angular_velocity_penalty * 0.05\n                  + action_penalty * 0.05\n                  + obstacle_reward * 0.1\n                  + fitness_reward * 0.2)\n        \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_reward\": obstacle_reward,\n            \"fitness_reward\": fitness_reward\n        }\n        \n        return reward, individual_reward",
        -84.9714998770971
    ]
}