{
    "reward": "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right (scaled)\n        distance_reward = pos.x / 200.0\n    \n        # Stability reward: penalizing large angle and angular velocity (reduced magnitude)\n        angle_penalty = -np.abs(state[0]) * 0.05\n        angular_velocity_penalty = -np.abs(state[1]) * 0.01\n    \n        # Smoothness reward: penalizing high-speed actions (scaled)\n        action_penalty = -np.sum(np.abs(action)) / len(action) * 0.1\n    \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.2\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.1 * angle_penalty\n                  + 0.05 * angular_velocity_penalty\n                  + 0.15 * action_penalty\n                  + 0.3 * obstacle_penalty)  # Higher weight given to distance reward and obstacle penalty\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty\n        }\n    \n        return reward, individual_reward",
    "fitness": -9.90103017578458,
    "iteration": 1,
    "sample": 4
}