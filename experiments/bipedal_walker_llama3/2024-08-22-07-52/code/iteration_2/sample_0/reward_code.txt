def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivizing movement to the right (scaled) and considering speed
    distance_reward = (pos.x / 200.0) * (1 - np.abs(action[1]) / 5.0)
    
    # Stability reward: penalizing large angle and angular velocity (reduced magnitude)
    stability_penalty = -np.sum([np.abs(state[i]) * 0.005 for i in range(2)])  # reduce magnitude
    
    # Smoothness reward: penalizing high-speed actions (scaled) with increased weight
    smoothness_reward = -(np.sum(np.abs(action)) / len(action)) * 0.3
    
    # Obstacle penalty: encouraging avoidance of obstacles with increased magnitude and considering multiple obstacles
    obstacle_penalty = -min([l.fraction for l in self.lidar]) * 0.4
    
    # Fitness score reward: incentivizing higher fitness scores
    fitness_reward = np.clip(state[2], 0, 1) * 0.5
    
    # Total reward: a weighted sum with adjusted weights
    reward = (distance_reward + stability_penalty + smoothness_reward + obstacle_penalty + fitness_reward)
    
    individual_reward = {
        "distance_reward": distance_reward,
        "stability_penalty": stability_penalty,
        "smoothness_reward": smoothness_reward,
        "obstacle_penalty": obstacle_penalty,
        "fitness_reward": fitness_reward
    }
    
    return reward, individual_reward