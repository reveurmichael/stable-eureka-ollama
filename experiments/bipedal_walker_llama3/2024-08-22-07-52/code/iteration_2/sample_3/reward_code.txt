def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivizing movement to the right (scaled)
    distance_reward = pos.x / 200.0

    # Stability reward: penalizing large angle and angular velocity (reduced magnitude)
    stability_penalty = -np.abs(state[0]) * 0.05
    angular_velocity_penalty = -np.abs(state[1]) * 0.01

    # Smoothness reward: penalizing high-speed actions (scaled)
    action_penalty = -np.sum(np.abs(action)) / len(action) * 0.03

    # Proximity to Lidar points: encouraging avoidance of obstacles
    obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.1

    # Fitness score: incentivizing good actions (increased magnitude)
    fitness_reward = np.sum(fitness_score) * 0.2

    # Total reward: a weighted sum with adjusted weights
    reward = (distance_reward + 0.15 * stability_penalty + 0.05 * angular_velocity_penalty
              + 0.03 * action_penalty + 0.1 * obstacle_penalty + fitness_reward)

    individual_reward = {
        "distance_reward": distance_reward,
        "stability_penalty": stability_penalty,
        "angular_velocity_penalty": angular_velocity_penalty,
        "action_penalty": action_penalty,
        "obstacle_penalty": obstacle_penalty,
        "fitness_reward": fitness_reward
    }

    return reward, individual_reward