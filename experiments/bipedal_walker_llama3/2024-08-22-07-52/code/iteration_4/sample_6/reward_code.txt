def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivizing movement to the right (scaled)
    distance_reward = pos.x / 200.0 * 1
    
    # Fitness score: re-scaled and weighted
    fitness_score_reduced = (fitness_score - np.mean(fitness_score)) / np.std(fitness_score) * 5
    
    # Smoothness reward: penalizing high-speed actions (scaled)
    action_penalty = -np.sum(np.abs(action)) / len(action) * 0.01
    
    # Proximity to Lidar points: encouraging avoidance of obstacles
    obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.05
    
    # Total reward: a weighted sum with adjusted weights
    reward = (distance_reward * 0.2
              + fitness_score_reduced * 0.3
              + action_penalty * 0.1
              + obstacle_penalty * 0.4)
    
    individual_reward = {
        "distance_reward": distance_reward,
        "fitness_score_reduced": fitness_score_reduced,
        "action_penalty": action_penalty,
        "obstacle_penalty": obstacle_penalty
    }
    
    return reward, individual_reward