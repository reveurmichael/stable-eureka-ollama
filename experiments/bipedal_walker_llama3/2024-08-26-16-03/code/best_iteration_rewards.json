{
    "0": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right\n        distance_reward = np.exp(pos.x / 400.0) - 1\n    \n        # Stability reward: penalizing large angle and angular velocity\n        angle_penalty = -np.abs(state[0])\n        angle_penalty = np.tanh(angle_penalty / 0.5)\n    \n        angular_velocity_penalty = -np.abs(state[1])\n        angular_velocity_penalty = np.tanh(angular_velocity_penalty / 0.5)\n    \n        # Smoothness reward: penalizing high-speed actions\n        action_penalty = -np.sum(np.abs(action)) / len(action)\n        action_penalty = np.tanh(action_penalty / 0.5)\n    \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar])\n        obstacle_penalty = np.exp(obstacle_penalty)  # Exponential scaling for higher penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.1 * angle_penalty\n                  + 0.1 * angular_velocity_penalty\n                  + 0.3 * action_penalty\n                  + 0.5 * obstacle_penalty)  # Higher weight given to obstacle penalty and action penalty\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty,\n        }\n    \n        return reward, individual_reward",
        -1.1007857425044199
    ],
    "1": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivize movement to the right with moderate magnitude\n        distance_reward = np.tanh(pos.x / 100.0)  # Tanh activation for bounded rewards\n    \n        # Simplified stability reward: reduce penalty for small angles and velocities\n        angle_stability_penalty = -np.abs(state[0]) ** 2\n        angular_velocity_stability_penalty = -np.abs(state[1]) ** 2\n        stability_reward = np.mean([angle_stability_penalty, angular_velocity_stability_penalty])\n    \n        # Smoothness reward: moderate penalty for high-speed actions\n        action_smoothness_penalty = -np.sum(np.abs(action)) / len(action)\n        smoothness_reward = np.tanh(action_smoothness_penalty / 0.5)\n    \n        # Proximity to Lidar points: encourage avoidance of obstacles with moderate magnitude\n        obstacle_avoidance_reward = -np.min([l.fraction for l in self.lidar])\n        obstacle_avoidance_reward = np.exp(obstacle_avoidance_reward)  # Exponential scaling for higher penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.3\n                  + stability_reward * 0.2\n                  + smoothness_reward * 0.1\n                  + obstacle_avoidance_reward * 0.4)\n        \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"stability_reward\": stability_reward,\n            \"smoothness_reward\": smoothness_reward,\n            \"obstacle_avoidance_reward\": obstacle_avoidance_reward,\n        }\n        \n        return reward, individual_reward",
        -4.079251257734441
    ],
    "2": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivize movement to the right with moderate magnitude\n        distance_reward = np.tanh(pos.x / 100.0)\n    \n        # Simplified stability reward: reduce penalty for small angles and velocities (scaled)\n        angle_stability_penalty = -np.abs(state[0]) ** 2 * 0.1  # Reduced penalty\n        angular_velocity_stability_penalty = -np.abs(state[1]) ** 2 * 0.05  # Reduced penalty\n        stability_reward = np.mean([angle_stability_penalty, angular_velocity_stability_penalty])\n    \n        # Smoothness reward: moderate penalty for high-speed actions (scaled)\n        action_smoothness_penalty = -np.sum(np.abs(action)) / len(action) * 0.2  # Scaled penalty\n        smoothness_reward = np.tanh(action_smoothness_penalty)\n    \n        # Obstacle avoidance reward: encourage avoidance of obstacles with moderate magnitude (bounded)\n        obstacle_avoidance_reward = -np.clip(np.min([l.fraction for l in self.lidar]), -1, 0)  # Bounded penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.25\n                  + stability_reward * 0.15\n                  + smoothness_reward * 0.05\n                  + obstacle_avoidance_reward * 0.55)\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"stability_reward\": stability_reward,\n            \"smoothness_reward\": smoothness_reward,\n            \"obstacle_avoidance_reward\": obstacle_avoidance_reward,\n        }\n    \n        return reward, individual_reward",
        -13.025602000765524
    ],
    "3": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivize movement to the right with moderate magnitude\n        distance_reward = np.tanh(pos.x / 100.0)\n    \n        # Simplified stability reward: reduce penalty for small angles and velocities (scaled)\n        angle_stability_penalty = -np.abs(state[0]) ** 2 * 0.05  # Reduced penalty\n        angular_velocity_stability_penalty = -np.abs(state[1]) ** 2 * 0.02  # Reduced penalty\n        stability_reward = np.mean([angle_stability_penalty, angular_velocity_stability_penalty])\n    \n        # Smoothness reward: moderate to high penalty for high-speed actions (scaled)\n        action_smoothness_penalty = -np.sum(np.abs(action)) / len(action) * 0.5  # Scaled penalty\n        smoothness_reward = np.tanh(action_smoothness_penalty)\n    \n        # Obstacle avoidance reward: encourage avoidance of obstacles with high magnitude (bounded)\n        obstacle_avoidance_reward = -np.clip(np.min([l.fraction for l in self.lidar]), -1, 0.8)  # Bounded penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.3\n                  + stability_reward * 0.2\n                  + smoothness_reward * 0.15\n                  + obstacle_avoidance_reward * 0.35)\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"stability_reward\": stability_reward,\n            \"smoothness_reward\": smoothness_reward,\n            \"obstacle_avoidance_reward\": obstacle_avoidance_reward,\n        }\n    \n        return reward, individual_reward",
        -100.27983202882669
    ],
    "4": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivize movement to the right with moderate magnitude\n        distance_reward = np.tanh(pos.x / 100.0)\n    \n        # Simplified stability reward: reduce penalty for small angles and velocities (scaled)\n        angle_stability_penalty = -np.abs(state[0]) ** 2 * 0.01  # Reduced penalty\n        angular_velocity_stability_penalty = -np.abs(state[1]) ** 2 * 0.005  # Reduced penalty\n        stability_reward = np.mean([angle_stability_penalty, angular_velocity_stability_penalty])\n    \n        # Smoothness reward: moderate to high penalty for high-speed actions (scaled)\n        action_smoothness_penalty = -np.sum(np.abs(action)) / len(action) * 0.2  # Scaled penalty\n        smoothness_reward = np.tanh(action_smoothness_penalty)\n    \n        # Obstacle avoidance reward: encourage avoidance of obstacles with high magnitude (bounded)\n        obstacle_avoidance_reward = -np.clip(np.min([l.fraction for l in self.lidar]), -1, 0.5)  # Bounded penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward * 0.4\n                  + stability_reward * 0.3\n                  + smoothness_reward * 0.2\n                  + obstacle_avoidance_reward * 0.1)\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"stability_reward\": stability_reward,\n            \"smoothness_reward\": smoothness_reward,\n            \"obstacle_avoidance_reward\": obstacle_avoidance_reward,\n        }\n    \n        return reward, individual_reward",
        -100.26701383047458
    ]
}