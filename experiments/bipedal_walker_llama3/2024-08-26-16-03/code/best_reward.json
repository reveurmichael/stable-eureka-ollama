{
    "reward": "    # Generated code by stable-eureka\n    def compute_reward(self, pos, action, state):\n        # Distance-based reward: incentivizing movement to the right\n        distance_reward = np.exp(pos.x / 400.0) - 1\n    \n        # Stability reward: penalizing large angle and angular velocity\n        angle_penalty = -np.abs(state[0])\n        angle_penalty = np.tanh(angle_penalty / 0.5)\n    \n        angular_velocity_penalty = -np.abs(state[1])\n        angular_velocity_penalty = np.tanh(angular_velocity_penalty / 0.5)\n    \n        # Smoothness reward: penalizing high-speed actions\n        action_penalty = -np.sum(np.abs(action)) / len(action)\n        action_penalty = np.tanh(action_penalty / 0.5)\n    \n        # Proximity to Lidar points: encouraging avoidance of obstacles\n        obstacle_penalty = -np.min([l.fraction for l in self.lidar])\n        obstacle_penalty = np.exp(obstacle_penalty)  # Exponential scaling for higher penalty\n    \n        # Total reward: a weighted sum with adjusted weights\n        reward = (distance_reward\n                  + 0.1 * angle_penalty\n                  + 0.1 * angular_velocity_penalty\n                  + 0.3 * action_penalty\n                  + 0.5 * obstacle_penalty)  # Higher weight given to obstacle penalty and action penalty\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"angle_penalty\": angle_penalty,\n            \"angular_velocity_penalty\": angular_velocity_penalty,\n            \"action_penalty\": action_penalty,\n            \"obstacle_penalty\": obstacle_penalty,\n        }\n    \n        return reward, individual_reward",
    "fitness": -1.1007857425044199,
    "iteration": 0,
    "sample": 6
}