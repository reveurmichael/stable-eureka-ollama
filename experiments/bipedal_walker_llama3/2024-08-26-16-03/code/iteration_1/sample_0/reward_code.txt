def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivizing movement to the right (re-scaled)
    distance_reward = 0.2 * np.tanh(pos.x / 200.0) - 1
    
    # Stability reward: penalizing large angle and angular velocity (re-scaled)
    angle_penalty = -np.abs(state[0]) * 0.8
    angle_penalty = np.tanh(angle_penalty / 0.5)
    
    angular_velocity_penalty = -np.abs(state[1]) * 0.8
    angular_velocity_penalty = np.tanh(angular_velocity_penalty / 0.5)
    
    # Smoothness reward: penalizing high-speed actions (re-scaled)
    action_penalty = -np.sum(np.abs(action)) / len(action) * 0.3
    action_penalty = np.tanh(action_penalty / 0.2)
    
    # Proximity to Lidar points: encouraging avoidance of obstacles (re-scaled)
    obstacle_penalty = -np.min([l.fraction for l in self.lidar]) * 0.5
    obstacle_penalty = np.exp(obstacle_penalty)  # Exponential scaling for higher penalty
    
    # Total reward: a weighted sum with adjusted weights
    reward = (distance_reward
              + 0.05 * angle_penalty
              + 0.05 * angular_velocity_penalty
              + 0.1 * action_penalty
              + 0.5 * obstacle_penalty)  # Higher weight given to obstacle penalty and action penalty
    
    individual_reward = {
        "distance_reward": distance_reward,
        "angle_penalty": angle_penalty,
        "angular_velocity_penalty": angular_velocity_penalty,
        "action_penalty": action_penalty,
        "obstacle_penalty": obstacle_penalty,
    }
    
    return reward, individual_reward