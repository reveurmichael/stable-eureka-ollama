def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivizing movement to the right (scaled)
    distance_reward = 2 * np.tanh(pos.x / 400.0) - 1
    
    # Stability and smoothness rewards: adjusted weights and scales
    angle_stability_penalty = -np.abs(state[0]) ** 0.5
    angular_velocity_stability_penalty = -np.abs(state[1]) ** 0.5
    action_smoothness_penalty = -np.sum(np.abs(action)) / len(action) ** 0.5
    
    # Obstacle penalty: adjusted scale and exponentiation
    obstacle_penalty = -np.min([l.fraction for l in self.lidar]) ** 2 * np.exp(-1)
    
    # Total reward: weighted sum with adjusted weights
    reward = (distance_reward + 0.25 * angle_stability_penalty
              + 0.25 * angular_velocity_stability_penalty
              + 0.5 * action_smoothness_penalty
              + 0.75 * obstacle_penalty)  # Higher weight given to obstacle penalty
    
    individual_reward = {
        "distance_reward": distance_reward,
        "angle_stability_penalty": angle_stability_penalty,
        "angular_velocity_stability_penalty": angular_velocity_stability_penalty,
        "action_smoothness_penalty": action_smoothness_penalty,
        "obstacle_penalty": obstacle_penalty,
    }
    
    return reward, individual_reward