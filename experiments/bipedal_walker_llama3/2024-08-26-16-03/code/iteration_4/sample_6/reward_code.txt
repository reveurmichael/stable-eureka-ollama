def compute_reward(self, pos, action, state):
    # Distance-based reward: incentivize movement to the right with moderate magnitude
    distance_reward = np.tanh(pos.x / 100.0)
    
    # Stability and smoothness rewards: increase penalty for high-speed actions and small angles
    angle_stability_penalty = -np.abs(state[0]) ** 2 * 0.1
    angular_velocity_stability_penalty = -np.abs(state[1]) ** 2 * 0.05
    stability_reward = np.mean([angle_stability_penalty, angular_velocity_stability_penalty])
    
    # Smoothness reward: moderate to high penalty for high-speed actions (scaled)
    action_smoothness_penalty = -np.sum(np.abs(action)) / len(action) * 1.0
    smoothness_reward = np.tanh(action_smoothness_penalty)
    
    # Exploration reward: penalize lack of exploration and obstacle avoidance
    exploration_penalty = -np.mean([l.fraction for l in self.lidar]) * 0.3
    exploration_reward = np.clip(exploration_penalty, -1, 0.5)  # Bounded penalty
    
    # Obstacle avoidance reward: encourage avoidance of obstacles with high magnitude (bounded)
    obstacle_avoidance_reward = -np.clip(np.min([l.fraction for l in self.lidar]), -1, 0.8)  # Bounded penalty
    
    # Total reward: a weighted sum
    reward = (distance_reward * 0.2
              + stability_reward * 0.3
              + smoothness_reward * 0.2
              + exploration_reward * 0.15
              + obstacle_avoidance_reward * 0.3)
    
    individual_reward = {
        "distance_reward": distance_reward,
        "stability_reward": stability_reward,
        "smoothness_reward": smoothness_reward,
        "exploration_reward": exploration_reward,
        "obstacle_avoidance_reward": obstacle_avoidance_reward,
    }
    
    return reward, individual_reward