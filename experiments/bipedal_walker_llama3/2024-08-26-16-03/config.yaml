eureka:
    backend: 'ollama'                   # 'ollama' or 'openai'
    model: 'llama3'                     # name of the model (from ollama or openai models)
    temperature: 0.8                    # the higher it is, the more random the output will be
    iterations: 5                       # number of total iterations
    samples: 8                          # number of samples to generate per iteration, they will run in parallel, so consider your HW     
    sleep_time_per_iteration: 5         # sleep time between iterations in minutes (only if using ollama to avoid cuda memory issues)
    use_initial_reward_prompt: true    # if available, use the initial reward prompt provided in the environment folder
    pretraining_with_best_model: false   # use best model weights for pretraining the next iteration set of models

environment:
    name: 'bipedal_walker'              # name of the environment folder
    max_episode_steps: 1600             # maximum number of steps per episode (truncate the episode)
    class_name: 'BipedalWalker'         # name of the gymnasium environment class
    kwargs: null                        # kwargs to pass to the environment class
    benchmark: 'BipedalWalker-v3'       # if benchmark available, train the agent with the same params to compare

experiment: 
    parent: 'experiments'               # parent folder where the experiments will be saved
    name: 'bipedal_walker_llama3'        # name of the experiment folder
    use_datetime: true                  # use datetime in the experiment folder name

rl:
    algo: 'ppo'                         # 'ppo', 'sac', 'dqn', 'td3', 'ddpg'               
    algo_params:                        # hyperparameters for the algorithm (it depends on the algorithm, same as in sb3)          
        policy: 'MlpPolicy'             
        learning_rate: 0.0003           
        n_steps: 2048
        batch_size: 64
        n_epochs: 10
        gamma: 0.999
        gae_lambda: 0.95
        clip_range: 0.2
        ent_coef: 0.0
        vf_coef: 0.5
        max_grad_norm: 0.5

    architecture:                       # architecture of the networks (it depends on the algorithm, same as in sb3)
        net_arch: {'pi': [64, 64], 'vf': [64, 64]}  
        activation_fn: 'ReLU'
        share_features_extractor: false

    training:
        torch_compile: true             # compile the model with torch.compile to be faster
        seed: 0                         # seed for reproducibility
        eval:
            seed: 5                     # seed for evaluation        
            num_episodes: 2             # number of episodes to evaluate
            num_evals: 10               # number of evaluations to perform during training
        total_timesteps: 1_000_000      # total timesteps to train the agent
        device: 'cuda'                  # 'cuda' or 'cpu'
        num_envs: 4                     # number of environments to run in parallel 
        state_stack: 1                  # number of frames to stack
        is_atari: false                 # if the environment is an atari game

    evaluation:
        seed: 10                        # seed for final evaluation
        num_episodes: 10                # number of episodes to evaluate
        save_gif: true                  # save gif of the best model