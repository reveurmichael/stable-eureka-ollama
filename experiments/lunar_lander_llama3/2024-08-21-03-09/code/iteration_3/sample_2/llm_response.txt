Based on the provided code, I can infer that your reward function should be something like this:

```
def compute_reward(self, state, m_power, s_power, terminated):
    # Reward for not crashing
    if not terminated:
        # Use a simple linear reward scheme
        reward = -1.0 * (state[0]**2 + state[1]**2)
        
    else:
        # When the episode is terminated, give a penalty to encourage exploration
        reward = -100.0
        
    return reward, {'main_engine': m_power, 'side_engine': s_power}
```

This reward function encourages the agent to stay close to the origin (i.e., not crash) by giving it a negative reward proportional to its distance from the origin. When the episode is terminated, it gives a penalty to encourage exploration and try new actions. The individual rewards for main and side engines are also included.