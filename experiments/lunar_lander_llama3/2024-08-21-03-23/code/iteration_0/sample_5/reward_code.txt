def compute_reward(self, state, m_power, s_power, terminated):
    # Reward structure:
    # 1. Lander position: closer is better (inverse distance)
    # 2. Main engine usage: less is better (negative reward)
    # 3. Orientation: closer to the helipad is better (orientation similarity)
    # 4. Velocity: lower is better (velocity magnitude)

    # Compute rewards
    pos_reward = -1 * np.square(state[0])  # inverse distance to the helipad
    main_engine_reward = -m_power  # negative reward for main engine usage
    orientation_reward = np.cos(state[3] - self.lander.angle)  # orientation similarity
    vel_reward = -np.linalg.norm(state[2:])  # velocity magnitude

    # Combine rewards
    reward = pos_reward + main_engine_reward + orientation_reward + vel_reward

    individual_reward = {
        'pos_reward': pos_reward,
        'main_engine_reward': main_engine_reward,
        'orientation_reward': orientation_reward,
        'vel_reward': vel_reward
    }

    return reward, individual_reward