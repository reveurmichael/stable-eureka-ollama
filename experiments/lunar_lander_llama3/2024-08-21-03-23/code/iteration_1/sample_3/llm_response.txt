The reward code!

Here's the relevant part of the code:
```python
def compute_reward(self, state, m_power, s_power, terminated):
    # Compute the reward based on the current state and actions
    if not terminated:
        # Reward for moving forward (x-axis)
        reward = -10.0 * np.square(state[3])  # vel.x
        if state[4] > 20:  # angle
            reward -= 1.0  # penalty for high angle

        # Reward for orientation (y-axis)
        reward -= 5.0 * np.square(state[2])  # vel.y
        if abs(state[1]) > 0.5:  # y position
            reward -= 5.0  # penalty for being far from the helipad

        # Reward for side engines (orientation-dependent)
        if s_power > 0:
            reward += 10.0 * np.square(s_power)

        # Add a small bonus for every step taken
        reward += 0.1
    else:
        # Terminal state, return a penalty
        reward = -1000.0

    individual_reward = {}
    if m_power > 0:
        individual_reward['main_engine'] = 10 * np.square(m_power)
    if s_power > 0:
        individual_reward['side_engines'] = 5 * np.square(s_power)

    return reward, individual_reward
```
In this code, the reward function is computed based on the current state of the lander and the actions taken. The reward is a combination of several components:

1. **Forward motion**: A penalty for not moving forward (x-axis) (-10.0 * np.square(state[3]))
2. **Orientation** (y-axis): A penalty for high angle or being far from the helipad
3. **Side engines**: A bonus for using side engines, dependent on their power level
4. **Main engine**: A bonus for using the main engine, also dependent on its power level

In the terminal state, a large penalty is returned.

The individual reward dictionary keeps track of the contributions from each type of action (main and side engines).