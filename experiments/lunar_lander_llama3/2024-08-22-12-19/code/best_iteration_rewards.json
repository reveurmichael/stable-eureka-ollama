{
    "0": [
        "    # Generated code by stable-eureka\n    def compute_reward(self, state, action, next_state, terminated):\n        # Unpack state variables\n        x, y, vx, vy, angle, angular_velocity, left_leg, right_leg = state\n    \n        # Distance-based reward: incentivize moving towards the landing pad\n        distance_reward = -np.sqrt(x**2 + y**2)\n        distance_reward = np.exp(distance_reward / 100.0) - 1\n    \n        # Height reward: keep the lander at a reasonable height\n        height_penalty = -(y - self.helipad_y) ** 2\n        height_penalty = np.tanh(height_penalty / 1.0)\n    \n        # Velocity penalty: discourage high velocities\n        velocity_penalty = -(np.abs(vx) + np.abs(vy)) ** 2\n        velocity_penalty = np.tanh(velocity_penalty / 1.0)\n    \n        # Fuel efficiency reward: penalize using the main engine\n        fuel_penalty = -1 if action == 2 else 0\n    \n        # Landing reward: bonus for successful landing\n        landing_reward = 0\n        if left_leg and right_leg:\n            landing_reward = 100 if abs(x) < 0.1 and abs(y) < 0.1 else 10\n    \n        # Total reward: a weighted sum of all components\n        reward = (distance_reward\n                  + 0.2 * height_penalty\n                  + 0.5 * velocity_penalty\n                  + 0.01 * fuel_penalty\n                  + landing_reward)\n    \n        # Apply a penalty if the episode is terminated (crash or out of bounds)\n        if terminated and not (left_leg and right_leg):\n            reward -= 100\n    \n        individual_reward = {\n            \"distance_reward\": distance_reward,\n            \"height_penalty\": height_penalty,\n            \"velocity_penalty\": velocity_penalty,\n            \"fuel_penalty\": fuel_penalty,\n            \"landing_reward\": landing_reward,\n        }\n    \n        return reward, individual_reward",
        -101.50855495476853
    ]
}